{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rake import Rake\n",
    "\n",
    "my_rake = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"We study two $Q$-state Potts models coupled by the product of their energy operators, in the regime $2 < Q \\le 4$ where the coupling is relevant. A particular choice of weights on the square lattice is shown to be equivalent to the integrable $a_3^{(2)}$ vertex model. It corresponds to a selfdual system of two antiferromagnetic Potts models, coupled ferromagnetically. We derive the Bethe Ansatz equations and study them numerically for two arbitrary twist angles. The continuum limit is shown to involve two compact bosons and one non compact boson, with discrete states emerging from the continuum at appropriate twists. The non compact boson entails strong logarithmic corrections to the finite-size behaviour of the scaling levels, the understanding of which allows us to correct an earlier proposal for some of the critical exponents. In particular, we infer the full set of magnetic scaling dimensions (watermelon operators) of the Potts model. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \" We present an unsupervised framework for simultaneous appearance-based object discovery, detection, tracking and reconstruction using RGBD cameras and a robot manipulator. The system performs dense 3D simultaneous localization and mapping concurrently with unsupervised object discovery. Putative objects that are spatially and visually coherent are manipulated by the robot to gain additional motion-cues. The robot uses appearance alone, followed by structure and motion cues, to jointly discover, verify, learn and improve models of objects. Induced motion segmentation reinforces learned models which are represented implicitly as 2D and 3D level sets to capture both shape and appearance. We compare three different approaches for appearance-based object discovery and find that a novel form of spatio-temporal super-pixels gives the highest quality candidate object models in terms of precision and recall. Live experiments with a Baxter robot demonstrate a holistic pipeline capable of automatic discovery, verification, detection, tracking and reconstruction of unknown objects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system performs dense 3d simultaneous localization', 33.5),\n",
       " ('induced motion segmentation reinforces learned models', 32.333333333333336),\n",
       " ('highest quality candidate object models', 23.083333333333332),\n",
       " ('simultaneous appearance-based object discovery', 15.25),\n",
       " ('3d level sets', 10.5),\n",
       " ('appearance-based object discovery', 10.25),\n",
       " ('unsupervised object discovery', 9.25),\n",
       " ('holistic pipeline capable', 9.0),\n",
       " ('gain additional motion-cues', 9.0),\n",
       " ('baxter robot demonstrate', 7.75),\n",
       " ('improve models', 6.333333333333333),\n",
       " ('motion cues', 6.0),\n",
       " ('automatic discovery', 5.0),\n",
       " ('unsupervised framework', 4.5),\n",
       " ('represented implicitly', 4.0),\n",
       " ('rgbd cameras', 4.0),\n",
       " ('spatio-temporal super-pixels', 4.0),\n",
       " ('mapping concurrently', 4.0),\n",
       " ('live experiments', 4.0),\n",
       " ('jointly discover', 4.0),\n",
       " ('visually coherent', 4.0),\n",
       " ('robot manipulator', 3.75),\n",
       " ('unknown objects', 3.666666666666667),\n",
       " ('putative objects', 3.666666666666667),\n",
       " ('robot', 1.75),\n",
       " ('objects', 1.6666666666666667),\n",
       " ('appearance', 1.0),\n",
       " ('precision', 1.0),\n",
       " ('verification', 1.0),\n",
       " ('form', 1.0),\n",
       " ('verify', 1.0),\n",
       " ('approaches', 1.0),\n",
       " ('reconstruction', 1.0),\n",
       " ('recall', 1.0),\n",
       " ('present', 1.0),\n",
       " ('detection', 1.0),\n",
       " ('capture', 1.0),\n",
       " ('spatially', 1.0),\n",
       " ('structure', 1.0),\n",
       " ('tracking', 1.0),\n",
       " ('shape', 1.0),\n",
       " ('learn', 1.0),\n",
       " ('compare', 1.0),\n",
       " ('find', 1.0),\n",
       " ('terms', 1.0),\n",
       " ('manipulated', 1.0),\n",
       " ('2d', 1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rake.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 181 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit my_rake = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
